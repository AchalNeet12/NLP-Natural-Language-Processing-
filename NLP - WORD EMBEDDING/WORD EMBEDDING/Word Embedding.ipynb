{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f5d7f6-421e-4653-b872-dcb89f888949",
   "metadata": {},
   "source": [
    "- Word2Vec is a popular technique in natural language processing (NLP) for word embedding, which involves mapping words or phrases to continuous vector spaces. This representation captures semantic meanings, relationships, and similarities between words, making it easier for machine learning models to understand language.\r\n",
    "\r\n",
    "### Key Concepts of Word2Vec\r\n",
    "\r\n",
    "### Word Embeddings:\r\n",
    "Word2Vec transforms words into dense vectors, where semantically similar words are represented by similar vectors. For example, the words \"king\" and \"queen\" will have vectors that are close together in the embedding space.\r\n",
    "Training Methods:\r\n",
    "\r\n",
    "### Continuous Bag of Words (CBOW): \r\n",
    "Predicts a target word from its surrounding context words. Given a set of context words, CBOW tries to find the most probable word that fits that context.\r\n",
    "Skip-gram: Predicts the context words given a target word. It focuses on the relationships between words by attempting to maximize the probability of context words given a specific word.\r\n",
    "\r\n",
    "### High-Dimensional Space:\r\n",
    "Word vectors are typically represented in a high-dimensional space (e.g., 100 to 300 dimensions). The higher the dimensions, the more information the vectors can potentially capture, but it also increases computational complexity.\r\n",
    "\r\n",
    "### Similarity and Analogy:\r\n",
    "Word2Vec allows for measuring the similarity between words using cosine similarity. It also enables performing analogies, such as \"king - man + woman = queen.\"\r\n",
    "\r\n",
    "### Advantages of Word2Vec\r\n",
    "Efficiency: It can efficiently process large datasets and learn high-quality word embeddings.\r\n",
    "Capturing Semantic Relationships: Word2Vec captures semantic relationships between words better than traditional one-hot encoding.\r\n",
    "\r\n",
    "### Transfer Learning: \r\n",
    "Pre-trained Word2Vec embeddings can be fine-tuned for specific tasks, saving time and computational resources.\r\n",
    "\r\n",
    "### Limitations of Word2Vec\r\n",
    "- Static Representations:\r\n",
    "Word2Vec generates static embeddings, meaning that each word has a single vector representation regardless of context. This can lead to issues with polysemy (words with multiple meanings).\r\n",
    "\r\n",
    "- Out-of-Vocabulary (OOV) Words:\r\n",
    "Words that were not in the training dataset will not have embeddings, limiting the model's ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92247e94-3770-4677-8725-3a9f080b309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language': [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
      " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
      " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
      "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
      "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
      "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
      "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
      " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
      "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
      "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
      " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
      " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
      " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
      "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
      "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
      " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
      " -0.00903996  0.005836    0.00939121  0.00350693]\n",
      "Words similar to 'language': [('is', 0.21617141366004944), ('processing', 0.04468921571969986), ('natural', 0.015034787356853485), ('a', 0.0019510718993842602), ('learning', -0.03283996880054474)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n",
    "    [\"Machine\", \"learning\", \"is\", \"fun\"],\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Get the vector for a word\n",
    "vector = model.wv['language']\n",
    "print(\"Vector for 'language':\", vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('language', topn=5)\n",
    "print(\"Words similar to 'language':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c879435-4045-4ac1-9b55-29f699c9ad33",
   "metadata": {},
   "source": [
    "# Summary\r\n",
    "Word2Vec is a foundational technique in NLP that facilitates the understanding of word semantics through vector representations. Despite its limitations, it has paved the way for more advanced embedding techniques, such as GloVe and contextual embeddings like BERT and ELMo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523cdf7-7652-4bb6-873e-35349672bb41",
   "metadata": {},
   "source": [
    "CBOW (Continuous Bag of Words) and Skip-gram are two architectures used in Word2Vec for generating word embeddings. Both approaches aim to learn vector representations of words based on their context, but they do so in different ways.\r\n",
    "\r\n",
    "# 1. Continuous Bag of Words (CBOW)\r\n",
    "How It Works: CBOW predicts a target word given a context of surrounding words. For instance, in the sentence \"The cat sits on the mat,\" if you take the context words \"The,\" \"cat,\" \"on,\" \"the,\" and \"mat,\" CBOW would predict the target word \"sits.\"\r\n",
    "\r\n",
    "- Mechanism:\r\n",
    "Input: A context (surrounding words).\r\n",
    "Output: The predicted target word.\r\n",
    "The model averages the vectors of the context words to generate a prediction for the target word.\r\n",
    "Example: Given the context words (\"The,\" \"cat,\" \"on,\" \"the,\" \"mat\"), CBOW would try to predict the word \"sits.\"\r\n",
    "\r\n",
    "- Strengths:\r\n",
    "Works well with smaller datasets.\r\n",
    "Tends to capture the global context of the sentence.\r\n",
    "\r\n",
    "# 2. Skip-gram\r\n",
    "How It Works: Skip-gram does the opposite of CBOW; it uses a target word to predict its surrounding context words. For example, given the target word \"sits,\" it would try to predict the context words around it.\r\n",
    "\r\n",
    "- Mechanism:\r\n",
    "Input: A target word.\r\n",
    "Output: The predicted context words.\r\n",
    "The model generates predictions for multiple context words based on the input target word.\r\n",
    "Example: Given the target word \"sits,\" Skip-gram might predict the context words (\"The,\" \"cat,\" \"on,\" \"the,\" \"mat\").\r\n",
    "\r\n",
    "- Strengths:\r\n",
    "More effective for larger datasets.\r\n",
    "Captures more information about the relationships between words, making it useful for understanding semantics.\r\n",
    "\r\n",
    "![image.png](attachment:70273b5c-779b-4279-a68f-bcda61390d72.png)\r\n",
    "\r\n",
    "Conclusion\r\n",
    "Both CBOW and Skip-gram are effective methods for learning word embeddings, and the choice between them often depends on the size of the training dataset and the specific application. CBOW tends to be faster and is suitable for smaller datasets, while Skip-gram excels in capturing relationships in larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a83202-ae4f-4336-bef5-c7cf03198a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
